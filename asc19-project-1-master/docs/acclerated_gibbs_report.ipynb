{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial acclerated Gibbs sampling\n",
    "\n",
    "* 조원: 김동욱, 김민우, 김민지\n",
    "* Paper: Fox Colin, and Albert Parker. “Accelerated Gibbs sampling of normal distributions using matrix splittings and polynomials.” Bernoulli 23.4B (2017): 3711-3743."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "* Our problem is sampling from normal distirubions.\n",
    "* Gibbs sampler is commonly used because of simple implementaion \n",
    "* However, the Gibbs sampler is not efficient for a massive models.\n",
    "* C. Fox and A.Parker(2017) proposed a generalized and accelerated Gibbs samplers which is fast for high-dimensional normal distributions.\n",
    "* In this presentation, we review the paper of C. Fox and A. Parker(2017) \"Accelerated Gibbs sampling of normal distributions using matrix splittings and polynomials\" -  Bernoulli 23.4B and show the reseult of the proposed algorithm re-written by **Julia**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem setting\n",
    "\n",
    "#### Problem\n",
    "1.  We want to compute a sample\n",
    "   $$\n",
    "   \\mathbf{y} \\sim \\mathbf{N}(0, A^{-1})\n",
    "   $$\n",
    "   , where $A$ is a given precision matrix. \n",
    "2.  An iterative samplers, such as Gibbs, are good option when the dimension is high becuase of it's inexpensive cost per itertion and small computer memroy requirements. \n",
    "3.  If the precision matrix $A$ is sparse with $\\mathcal{O}(n)$ non-zero elements, iterative methods cost only about $2n$ flops per iteration.\n",
    "4.  So, our goal is find an **Iterative method which converges in small iterations, for example, significantly less than $\\mathcal{O}(n^2)$ iterations**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-iterative sampler: Cholesky factorization\n",
    "\n",
    "<center><img src=\"img/algorithm1.png\"></center>\n",
    "\n",
    "* This is a baseline algorithm and non-iterative. \n",
    "* Cholesky factorizing requires that the precision matrix and the Cholesky factor be stored in computer memory, which can be prohibitive for large problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gibbs sampling from a Normal Distribution\n",
    "\n",
    "<center><img src=\"img/algorithm2.png\"></center>\n",
    "\n",
    "* The iteration can be written in the matrix form,\n",
    "$$\n",
    "    \\mathbf{y}^{(k+1)} = -\\mathbf{D}^{-1}\\mathbf{L}\\mathbf{y}^{(k+1)} - \\mathbf{D}^{-1}\\mathbf{L}^T\\mathbf{y}^{(k)} + \\mathbf{D}^{-1/2}\\mathbf{z}^{(k)}\n",
    "    \\tag{1}\n",
    "$$\n",
    ", where $\\mathbf{z}^{(k)} \\sim N(0, \\mathbf{I})$, $\\mathbf{D} = \\text{diag}(\\mathbf{A})$, and \n",
    "$\\mathbf{L}$ is the strictly lower triangluar part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Iterative Methods\n",
    "* The iterative method for solving $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ split the matrix \n",
    "$\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$, where $\\mathbf{M}$ is invertible and easy to invert.\n",
    "\n",
    "* Then $\\mathbf{A}\\mathbf{x} = \\mathbf{M}\\mathbf{x} - \\mathbf{N}\\mathbf{x} = \\mathbf{b}$ or\n",
    "    $$\n",
    "        \\mathbf{x} = \\mathbf{M}^{-1}\\mathbf{N}\\mathbf{x} - \\mathbf{M}^{-1}\\mathbf{b}.\n",
    "    $$\n",
    "\n",
    "* Thus a solution to $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ is a fixed point of iteration\n",
    "    $$\n",
    "        \\mathbf{x}^{(k+1)} = \\mathbf{M}^{-1}\\mathbf{N}\\mathbf{x}^{(k)} - \\mathbf{M}^{-1}\\mathbf{b}. \\tag{2}\n",
    "    $$\n",
    "* The iterative solution is convergent(i.e., $\\mathbf{x}^{(k)} \\rightarrow \\mathbf{A}^{-1}\\mathbf{b}$)\n",
    "if and only if \n",
    "$$\\mathcal{\\rho}(\\mathbf{M}^{-1}\\mathbf{N}) < 1$$ \n",
    ",where $\\mathcal{\\rho}(\\cdot)$ is the spectral radius of a matrix. \n",
    "* Let the error at step $k$ be $\\mathbf{e}^{(k+1)} =\\mathbf{x}^{(k+1)} - \\mathbf{A}^{-1}\\mathbf{b}.$\n",
    "Then it's well known(e.g. Axelsson) that\n",
    "$$\n",
    "    \\lim_{k \\to \\infty}\n",
    "    \\bigg(\\frac{\\|\\mathbf{e}^{(k+1)}\\|_2}{\\|\\mathbf{e}^{(0)}\\|_2}\\bigg)^{1/k}  \n",
    "    = \\mathcal{\\rho}(\\mathbf{M}^{-1}\\mathbf{N})\n",
    "    \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting iterative linear solvers\n",
    "\n",
    "* For a symmetric matrix $\\mathbf{A}$, let $\\mathbf{A} = \\mathbf{L} + \\mathbf{D} + \\mathbf{L}^T$\n",
    ", where $\\mathbf{L}$ is a strictly lower triangular part and $\\mathbf{D}$ is a diagonal part.\n",
    "\n",
    "|Splitting|$\\mathbf{M}$|Convergence|\n",
    "|-------------|--------------|:-----------:|\n",
    "|Richardson|$\\frac{1}{\\omega}\\mathbf{I}$|$0 < \\omega < \\frac{2}{\\rho(\\mathbf{A})}$|\n",
    "|Jacobi|$\\mathbf{D}$|$\\mathbf{A}$ strictly diagonally dominant|\n",
    "|Gauss-Seidel(GS)|$\\mathbf{D} + \\mathbf{L}$|always|\n",
    "|SOR|$\\frac{1}{\\omega}\\mathbf{D} + \\mathbf{L}$|$0 < \\omega < 2$|\n",
    "|SSOR|$\\frac{\\omega}{2 - \\omega}\\mathbf{M}_{\\text{SOR}}\\mathbf{D}^{-1}\\mathbf{M}_{\\text{SOR}}^T$|$0 < \\omega < 2$|\n",
    "\n",
    "* For the Gauss-Seidel method, (2) becomes\n",
    "\n",
    "    $$\n",
    "        \\mathbf{x}^{(k+1)} = -\\mathbf{D}^{-1}\\mathbf{L}\\mathbf{x}^{(k+1)} - \\mathbf{D}^{-1}\\mathbf{L}^T\\mathbf{x}^{(k)} + \\mathbf{D}^{-1}\\mathbf{b}. \\tag{4}\n",
    "    $$\n",
    "    \n",
    "    Note that the gibbs sampler iteration (1) and linear solver's (4) are almost same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Equivalence between iterative linear solvers and Gibbs samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalence of linear solvers and Gibbs samplers\n",
    "##### Theorem 1. (Equivalence of iterative linear solvers and Gibbs samplers)\n",
    "\n",
    ">Let $\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$ be a splitting with $\\mathbf{M}$ invertible, and let\n",
    "$\\pi(\\cdot)$ be some fixed probability distribution with zero mean and fixed non-zero covariance.\n",
    "For any fixed vector $\\mathbf{b}$, and random vectors \n",
    "$\\mathbf{c}^{(k)} \\overset{\\text{i.i.d.}}{\\sim} \\pi$, $k = 0, 1, 2, \\dots,$ \n",
    "the stationary linear iteration\n",
    "$$\n",
    "    \\mathbf{x}^{(k+1)} = \\mathbf{M}^{-1}\\mathbf{N}\\mathbf{x}^{(k)} + \\mathbf{M}^{-1}\\mathbf{b}\n",
    "$$\n",
    "converges, with $\\mathbf{x}^{(k)} \\to \\mathbf{A}^{-1}\\mathbf{b}$ as $k \\to \\infty$ whatever\n",
    "the initial vector $\\mathbf{x}^{(0)}$, if and only if there exists a distribution $\\Pi$ such that\n",
    "the stochastic iteration\n",
    "$$\n",
    "    \\mathbf{y}^{(k+1)} = \\mathbf{M}^{-1}\\mathbf{N}\\mathbf{y}^{(k)} + \\mathbf{M}^{-1}\\mathbf{c}^{(k)} \\tag{5}\n",
    "$$\n",
    "converges in distribution on $\\Pi$, with $\\mathbf{y}^{(k)} \\overset{\\mathcal{D}}{\\to} \\Pi$ as\n",
    "$k \\to \\infty$ whatever the initial state $\\mathbf{y}^{(0)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from normal distributions using matrix splittings\n",
    "##### Theorem 2. (Convergence of first and second moments)\n",
    "\n",
    ">Let $\\mathbf{A}$ be SPD, $\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$ be a convergent splitting,\n",
    "$\\mathbf{\\mu}$ a fixed vector, and $\\pi(\\cdot)$ a fixed probability distribution with finite mean\n",
    "$\\mathbf{\\nu}$ and non-zero covariance $\\mathbf{V}$. Consider the stochastic iteration (5) where\n",
    "$\\mathbf{c}^{(k)} \\overset{\\text{i.i.d.}}{\\to}\\pi$, $k = 0, 1, 2, \\dots .$ Then, whatever the starting\n",
    "state $\\mathbf{y}^{(0)}$, the followings are equivalent:\n",
    "* $\\textbf{E}[\\mathbf{c}^{(k)}] = \\mathbf{\\nu}$ and \n",
    "$\\textbf{Var}(\\mathbf{c}^{(k)}) = \\mathbf{V} = \\mathbf{M}^T + \\mathbf{N}$;\n",
    "* the iterates $\\mathbf{y}^{(k)}$ converges in distribution to some distribution $\\Pi$ that\n",
    "has mean $\\mathbf{\\mu} = \\mathbf{A}^{-1}\\mathbf{\\nu}$ and covariance matrix $\\mathbf{A}^{-1}$;\n",
    "in particular $\\textbf{E}[\\mathbf{y}^{(k)}] \\to \\mathbf{\\mu}$ and \n",
    "$\\textbf{Var}(\\mathbf{y}^{(k)}) \\to \\mathbf{A}^{-1}$ as $k \\to \\infty$.\n",
    "\n",
    "\n",
    "* This theorem shows blue **how to design** the noise distribution $\\pi$ so that the limit distribution $\\Pi$ has a desired mean $\\mu$ and covariance $\\Sigma = \\mathbf{A}^{-1}$.\n",
    "* If we set $\\pi = \\text{N}(\\mathbf{\\nu}, \\mathbf{V})$ then the following are equivalent:\n",
    "    1. $\\mathbf{V} = \\mathbf{M}^T + \\mathbf{N}$; \n",
    "    2. $\\mathbf{y}^{(k)} \\overset{\\mathcal{D}}{\\to} \\mathbf{N}(\\mathbf{\\mu}, \\mathbf{A}^{-1})$\n",
    "    , where $\\mathbf{\\mu} = \\mathbf{A}^{-1}\\mathbf{\\nu}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The followings are algorithms of Iterative Samplers and SSOR sampler. Note that for the SSOR sampler, $M_{\\text{SOR}}$ calculated implicitly.\n",
    "\n",
    " Iterative samper |  SSOR sampler\n",
    ":-------------------------:|:-------------------------:\n",
    "![](img/algorithm3.png)  |  ![](img/algorithm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proposed methods and Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accleration of linear solvers by polynomials\n",
    "* Most of the results in this section follows from the [Axelsson, Iterative Solution Methods](https://www.cambridge.org/core/books/iterative-solution-methods/934B9797ADF03EBC377B20622DADE822)\n",
    "* For a set of *acceleration parameters* $\\{ \\{\\alpha_k\\}, \\{\\tau_k\\}\\}$, let's introduce the second order iteration\n",
    "    $$ \\mathbf{x}^{(k+1)} = (1-\\alpha_k)\\mathbf{x}^{(k-1)} + \\alpha_k \\mathbf{x}^{(k)} +\n",
    "       \\alpha_k\\tau_k\\mathbf{M}^{-1}(\\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)}). \\tag{6}\n",
    "    $$\n",
    "\n",
    "* At the first step, $\\alpha_0 = 1$ and $\\mathbf{x}^{(1)} = \\mathbf{x}^{(0)} + \\tau_0\\mathbf{M}^{-1}(\\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(0)})$.\n",
    "* Next, generate a (k+1)st order polynomial $P_{k+1}$ recursively as\n",
    "    $$\n",
    "    P_{k+1}(\\lambda) = (\\alpha_k - \\alpha_k\\tau_k\\lambda)P_k(\\lambda) + (1-\\alpha_k)P_{k-1}(\\lambda). \\tag{7}\n",
    "    $$\n",
    "\n",
    "* Then the $k$-step error $\\mathbf{e}^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{A}^{-1}\\mathbf{b}$ may be written as \n",
    "    $$\n",
    "    \\mathbf{e}^{(k+1)} = P_k(\\mathbf{M}^{-1}\\mathbf{A}) \\mathbf{e}^{(0)}  \\tag{8}\n",
    "    $$\n",
    "  , which can be compared directly to (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* When estimates of the extreme eigenvalues $\\lambda_{\\min}$, $\\lambda_{\\max}$ of \n",
    "$\\mathbf{M}^{-1}\\mathbf{A}$ are available($\\lambda_{\\min}$, $\\lambda_{\\max}$ are real when\n",
    "$\\mathbf{M}, \\mathbf{N}$ are symmetric), then following coefficients\n",
    "$\\{ \\{\\alpha_k\\}, \\{\\tau_k\\}\\}$ generate the scaled **Chebyshev polynomials**\n",
    "\n",
    "$$  \\tau_k = \\frac{2}{\\lambda_{\\max} + \\lambda_{\\min}}, \\quad\n",
    "    \\beta_k = \\bigg( \\frac{1}{\\tau_k} - \\beta_{k-1}\n",
    "    \\bigg( \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{4}\n",
    "    \\bigg)^2 \\bigg)^{-1}, \\quad\n",
    "    \\alpha_k = \\frac{\\beta_k}{\\tau_k},\n",
    "$$\n",
    "\n",
    "where $\\alpha_0 = 1$ and $\\beta_0 = \\tau_0$.\n",
    "\n",
    "Remarks:\n",
    ">* We denote the $k$-th order Chebyshev polynomial as $\\mathcal{Q}_k$.\n",
    ">*  Note that these parameters are independent of the iterates $\\{\\mathbf{x}^{(k)}\\}$.\n",
    ">* $\\mathbf{M}$ is required to be symmetric, applying Chebyshev acceleration to SSOR is a common paring.\n",
    "\n",
    "\n",
    "* From *Axelsson*,\n",
    "\n",
    "$$  \\mathcal{Q}_k = \\underset{{P_k \\in \\mathbb{P}_k}}{\\text{argmin}}\n",
    "    \\bigg(\n",
    "    \\underset{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]}{\\max} |P_k(\\lambda)|\n",
    "    \\bigg),\n",
    "$$\n",
    "\n",
    "where $\\mathbb{P}_k$ is a space of $k$th order polynomials. The optimal value is\n",
    "\n",
    "$$ \\underset{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]}{\\max} |\\mathcal{Q}_k(\\lambda)|\n",
    "    = \\frac{2\\sigma^k}{1 + \\sigma^{2k}}, \\quad\n",
    "    \\text{where }\n",
    "    \\sigma = \\frac{1 - \\sqrt{\\lambda_{\\min}/\\lambda_{\\max}}}\n",
    "    {1 + \\sqrt{\\lambda_{\\min}/\\lambda_{\\max}}} \\in [0, 1).\n",
    "$$\n",
    "\n",
    "From (8), \n",
    "$\\mathbf{e}^{(k+1)} = \\mathcal{Q}_k(\\mathbf{M}^{-1}\\mathbf{A})\\mathbf{e}^{(0)}$ and\n",
    "then the asymptotic convergence factor is bounded above by\n",
    "\n",
    "$$ \\lim_{k \\to \\infty} \\{\\max |\\mathcal{Q}_k(\\lambda)|\\}^{1/k} = \\sigma .$$\n",
    "\n",
    "* *Axelsson* also shows the convergence factor of non-accelerated iterative solver is\n",
    "bounded below by $$\\rho = \\frac{1 - \\lambda_{\\min}/\\lambda_{\\max}}{1 + \\lambda_{\\min}/\\lambda_{\\max}}.$$\n",
    "Then $\\sigma < \\rho$ always holds, so the Chebyshev acceleration **really accelerate** a calculation procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceleration of Gibbs sampling by polynomials\n",
    "\n",
    "* By slightly chainging (6), we can consider the second order stochastic iteration\n",
    "    $$  \\mathbf{y}^{(k+1)} = \n",
    "        (1-\\alpha_k)\\mathbf{y}^{(k-1)} + \\alpha_k \\mathbf{y}^{(k)} +\n",
    "        \\alpha_k\\tau_k\\mathbf{M}^{-1}(\\mathbf{c}^{(k)} - \\mathbf{A}\\mathbf{y}^{(k)}). \\tag{9}\n",
    "    $$\n",
    "    \n",
    "    Only diffrenet thing is the vector $\\mathbf{b}$ \n",
    "    has been replaced by a random vector $\\mathbf{c}^{(k)}$.\n",
    "* Next, introduce some coefficients $a_k, b_k, \\kappa_k$ defined by\n",
    "    $a_k = (2-\\tau_k)/\\tau_k + (b_k-1)(1/\\tau_k + 1/\\kappa_k - 1)$, \n",
    "    $b_k = 2\\kappa_k(1-\\alpha_k)/(\\alpha_k \\tau_k) + 1$, \n",
    "    $\\kappa_{k+1} = \\alpha_k \\tau_k + (1-\\alpha_k)\\kappa_k$, and $\\kappa_1 = \\tau_0$.\n",
    "\n",
    "* Also, suppose that $\\{\\{\\alpha_k\\}, \\{\\tau_k\\}\\}$ that are independent of \n",
    "    $\\{\\mathbf{x}^{(k)}\\}$.\n",
    "* Then (9) converges in distribution to our targe distribution if the polynomial accerated linear solver converges.\n",
    "\n",
    "\n",
    "\n",
    "### Main Results\n",
    "##### Theorem 3. (Accelerated Gibbs sampler)\n",
    ">Let $\\mathbf{A}$ be SPD and $\\mathbf{A} = \\mathbf{M} - \\mathbf{N}$ be a symmetric splitting. Define an noise vectors \n",
    "$\\mathbf{c}^{(k)} \\overset{ind.}{\\sim} (\\mathbf{\\nu}, a_k \\mathbf{M} + b_k \\mathbf{N})$.\n",
    ">1. If the accelerated linear solver (7) converges to $\\mathbf{A}^{-1}\\mathbf{b}$\n",
    "    then the accelerated stochastic iteration (14)\n",
    "    converges to a distribution with moments $(\\mathbf{A}^{-1}\\nu, \\mathbf{A}^{-1}).$ \n",
    "    Furthermore, if the $\\{\\mathbf{c}^{(k)}\\}$ are normal, then\n",
    "    $$  \\mathbf{y}^{(k)} \\overset{\\mathcal{D}}{\\to} \n",
    "        \\mathbf{N}(\\mathbf{\\mu} = \\mathbf{A}^{-1}\\nu, \\mathbf{A}^{-1}).$$\n",
    "2. $\\mathbf{E}(\\mathbf{y}^{(k)})-\\mathbf{A}^{-1}\\nu \n",
    "= P_k(\\mathbf{M}^{-1}\\mathbf{A})(\\mathbf{E}(\\mathbf{y}^{(0)})-\\mathbf{A}^{-1}\\nu)\\to 0$\n",
    "3. $\\mathbf{Var}(\\mathbf{y}^{(k)})-\\mathbf{A}^{-1} \n",
    "= P_k(\\mathbf{M}^{-1}\\mathbf{A})(\\mathbf{Var}(\\mathbf{y}^{(0)})-\\mathbf{A}^{-1})\\to 0$\n",
    "\n",
    "\n",
    "\n",
    "* Chebyshev polynomial accelerated normal sampler is guaranteed to\n",
    "converge faster than any other acceleration scheme that has the parameters \n",
    "$\\{ \\alpha_k, \\tau_k \\}$ independent of the iterates $\\{\\mathbf{y}^{(k)}\\}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computed examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6.1 in the paper\n",
    "\n",
    "#### 1. Data\n",
    "\n",
    "* A *first order locally linear* sparse precision matrix $\\mathbf{A} \\in \\mathbb{R}^{n^2 \\times n^2}$ is defined by\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_{ij}=\n",
    "10^{-4}\\delta_{ij} + \n",
    "\\begin{cases}\n",
    "n_i, \\quad &\\text{if } i = j, \\\\\n",
    "-1,\\quad &\\text{if } i \\neq j \\text{ and } \\|s_i - s_j\\|_2 \\leq 1, \\\\\n",
    "0 \\quad &\\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* The discrete points $\\{s_i\\}$ are on a regular $n \\times n$ lattice over the two dimensional domain $\\mathcal S = [1, n] \\times [1, n]$.\n",
    "  So, $s_i$ is a point of $(j, k)$ where $i = j + n \\cdot (k-1)$. For example, $s_1 = (1, 1)$, $s_{10} = (10, 1)$ and $s_{23} = (3, 3)$ when $n = 10$.\n",
    "\n",
    "* We use a sparse structure to implement. Note that a size of A increases very fast for the terms of $n^4$. For the detail implementation, see `\"src/matrixgen.jl\"`. With our code, $A$ matrix with $n=512$ can also be handled, however dense matrix of size $512^4 = 2^{36}$ causes a `out of memory()` problem.\n",
    "\n",
    "* The following plot is a sparsity pattern of $\\mathbf{A}$ when $n = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1m                    Sparsity Pattern\u001b[22m\n",
       "\u001b[90m       ┌──────────────────────────────────────────┐\u001b[39m    \n",
       "     \u001b[90m1\u001b[39m\u001b[90m │\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠘\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \u001b[31m> 0\u001b[39m\n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠉\u001b[39m\u001b[34m⢆\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m \u001b[34m< 0\u001b[39m\n",
       "      \u001b[90m │\u001b[39m\u001b[34m⠒\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠱\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⢢\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[34m⠣\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⠤\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠱\u001b[39m\u001b[34m⣀\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠱\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⢄\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⡄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠘\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[34m⠑\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠱\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠉\u001b[39m\u001b[34m⢆\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[34m⠒\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⢢\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠣\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠛\u001b[39m\u001b[35m⣤\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⠤\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠱\u001b[39m\u001b[34m⣀\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⢄\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⡄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠛\u001b[39m\u001b[35m⣤\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠘\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[34m⠑\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠉\u001b[39m\u001b[34m⢆\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[34m⠒\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠛\u001b[39m\u001b[35m⣤\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⢢\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠣\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⠤\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠱\u001b[39m\u001b[34m⣀\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⢆\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⢄\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⡄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠘\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[34m⠑\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⢆\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠉\u001b[39m\u001b[34m⢆\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[34m⠒\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⢢\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠣\u001b[39m\u001b[34m⢄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⢆\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⠤\u001b[39m\u001b[90m│\u001b[39m    \n",
       "      \u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠱\u001b[39m\u001b[34m⣀\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[34m⡀\u001b[39m\u001b[0m⠀\u001b[90m│\u001b[39m    \n",
       "   \u001b[90m100\u001b[39m\u001b[90m │\u001b[39m\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[0m⠀\u001b[34m⠑\u001b[39m\u001b[34m⡄\u001b[39m\u001b[0m⠀\u001b[34m⠈\u001b[39m\u001b[35m⠻\u001b[39m\u001b[35m⣦\u001b[39m\u001b[90m│\u001b[39m    \n",
       "\u001b[90m       └──────────────────────────────────────────┘\u001b[39m    \n",
       "\u001b[90m       1\u001b[39m\u001b[90m                    \u001b[39m\u001b[90m                    100\u001b[39m\n",
       "\u001b[0m                        nz = 460"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"../src/matrixgen.jl\")\n",
    "import UnicodePlots: spy\n",
    "A = laplacematrix(10)\n",
    "spy(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Algorithm implementation\n",
    "* Our major purpose is implementation of `Cheby-acclerated sampling`. It's detail algorithm given in the supplementary of the paper. See `algorithm-chebysampler.png` in the current directory. Also, we implements a lot of Iterative Solvers and related samplers follow the flow of a SSOR sampler algorithm.\n",
    "\n",
    "* Writing down direct mathmatical expression raises **huge time delay and memory allocation problem**, Even few attemptions for reduction memory(Sparse, using basic matrix inversion operations) were failed. `k3_ssor` is what we implemented and it's performance is poor relative to `ssor` from `IterativeSolvers.jl`.\n",
    "\n",
    "|Example of Direct implementation of the Algorithm, Algorithm3  |  Benchmark results|\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|![](img/proto_performance_3.png)  |  ![](img/proto_performance.png)|\n",
    "\n",
    "##### Algorithm structure design\n",
    "We benchmark Julia itterative solver algorithms and figure out how to reduce computing time and memory allocations.  \n",
    "\n",
    "* The key for reducing time and memory is to declare all variables used in the update process in advance so that there is no additional memory allocation for them, define the new structure to access to the precision matrix for subsequent operations and implement all operations elementwisely or using forward and backward computations.\n",
    "* For this purpose, we follow 'IterativeSolver.jl''s *DiagonalIndices structure* and sparse based inplacement operation.\n",
    "* In julia sparse package, [SparseMatricCSC](https://github.com/JuliaLang/julia/blob/46ce4d79337bdd257ee2e3d2f4bb1c55ff0a5030/stdlib/SparseArrays/src/sparsematrix.jl#L11-L18) type is only storage 3 types of vectors; colum pointers, none zero values and row values\n",
    "* Itterative solver use the design Diagonal structure and store one more array ; Diagonal indices of none zero values.\n",
    "* Using Diagonal structure, Forward and backward operation can be computed without declaring new Lower or Upper Triangluar matrix.\n",
    "* For the detail implementation, see `\"src/cheby.jl\"`, `\"src/cheby_sampler.jl\"`.\n",
    "* Below figures are performance comparison between itteration solvers from `IterativeSolvers.jl` vs `implemented solvers`. Our algorithms have more advantage in saving memory allocations over 10 times compared with itterative solver and also faster than Julia itteration solver package.\n",
    "    \n",
    " |SOR |  SSOR|\n",
    "|:-------------------------:|:-------------------------:|\n",
    "|![](img/compare_performance_2.png)  |  ![](img/compare_performance.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implemented Accelerated Solvers.\n",
    "\n",
    "* We implement a Iterative Solvers: `Gauss-Seidel`, `SSOR`, `SOR`, `Cheby-SSOR` and **Iterative Samplers**: `SSOR-sampler`, `Cheby-acclerated Sampler`. For the detail implemtation, see `\"../src/cheby.jl\"` and `\"cheby_sampler.jl\"`\n",
    "* Following plot shows the accuracy of Cheby-SSOR. \n",
    "* We calculate `xtrue` by `A \\ b` from `julia`. And then plot the error between `xtrue` and our $\\mathbf{x}^{(k)}_{\\text{chebySSOR}}$, $\\|x_{\\text{true}} - \\mathbf{x}^{(k)}\\|_2$. For a detail implementaion, see `\"results/graph-cheby-error.ipynb\"`.\n",
    "* Note that the error decreases in log-scale.\n",
    "\n",
    "![](../results/graph_chebyssor_err.png)\n",
    "\n",
    "* Followings are the `@benchmark` comparing between SSOR of `IterativeSolvers.jl` and Cheby-accelerated SSOR which we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IterativeSolvers\n",
    "include(\"../src/cheby_dw.jl\")\n",
    "A = laplacematrix(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  3.74 MiB\n",
       "  allocs estimate:  245210\n",
       "  --------------\n",
       "  minimum time:     152.768 ms (0.00% GC)\n",
       "  median time:      164.098 ms (0.00% GC)\n",
       "  mean time:        163.219 ms (0.09% GC)\n",
       "  maximum time:     172.918 ms (0.52% GC)\n",
       "  --------------\n",
       "  samples:          31\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_ = copy(b)\n",
    "@benchmark ssor(A, b_, 1.6641, maxiter=61300)  # ssor from IterativeSolvers.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.009268 seconds (541 allocations: 752.641 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  36.89 KiB\n",
       "  allocs estimate:  33\n",
       "  --------------\n",
       "  minimum time:     1.478 ms (0.00% GC)\n",
       "  median time:      1.716 ms (0.00% GC)\n",
       "  mean time:        1.735 ms (0.18% GC)\n",
       "  maximum time:     3.885 ms (58.21% GC)\n",
       "  --------------\n",
       "  samples:          2878\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_ = copy(b)\n",
    "@time λ_max,λ_min = eigMm(A, 1.6641)\n",
    "@benchmark k3_CB_ssor(A, b_, 1.6641, λ_max, λ_min, 640) # our implemented cheby-accelerated ssor solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see that implemented **Cheby-acceleraed SSOR** is about 15 times faster than *SSOR* of IterativeSolvers.jl including a time for calculating maximum and minmum eigen values of $M^{-1}A$.\n",
    "* Also, we note that the memory efficiency of our accelerated SSOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Actual calculation by Iterative Solvers\n",
    "* In this section we reproduce *Table 3* in the paper.\n",
    "* We solve $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ where $\\mathbf{A}$ is a $n=10$ matrix from the previous sextion and $\\mathbf{b} \\sim \\mathbf{N}(\\mathbf{0}, \\mathbf{I}_{100})$ is a randomly generated for some fixed random seed.\n",
    "* Following table is a summary of calculation results. Each solver was run until the residual became sufficiently small, $\\|\\mathbf{b} - \\mathbf{A}\\mathbf{x}^{(k)}\\|_2 < 10^{-8}$.\n",
    "* Reproduced results are given in `\"results/count_floap.ipynb\"`, `\"results/count_iter.ipynb\"` and included `.jl` codes in `\"src/\"` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Solver|$\\omega$____|Number of iterations|Flops/iteration|Total Flops________|\n",
    "|-------------|--------|:--------------------|------|--------------------|\n",
    "|Richardson   | 1      |DNC                |1120 |-  |\n",
    "|Jacobi       | -      |$6.92 \\times 10^5$ |1320 |$9.13 \\times 10^8$   |\n",
    "|Gauss-Seidel | -      |$3.20 \\times 10^5$ |1580 |$5.05 \\times 10^8$   |\n",
    "|SSOR         | 1.6641 |$6.13 \\times 10^4$ |3240 |$1.99 \\times 10^8$   |\n",
    "|SOR          | 1.9852 |1668               |1580 |$2.64 \\times 10^6$   |\n",
    "|Cheby-SSOR   | 1      |1021               |3300 |$3.37 \\times 10^6$   |\n",
    "|Cheby-SSOR   | 1.6641 |636                |3300 |$2.10 \\times 10^6$   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Following Table is a *Table 3* from the paper for the comparison with our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/Table3paper.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mathcal{\\rho}(\\mathbf{M}^{-1}\\mathbf{N})$ is a spectral radius of splitting matrices which implies a convergent speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Empirical variances error plotting\n",
    "* We reproduce the `Figure 2` in the paper.\n",
    "* Relative error in covariance $\\|\\mathbf{A}^{-1} - \\mathbf{S}_y^{(k)}\\|_2/\\|\\mathbf{A}^{-1}\\|_2$ versus number of iterations for a sampler implemented with `SSOR` and ω = 1, `SSOR` with optimal relaxation ω = 1.6641, and `SSOR with Chebyshev acceleration`. To numerically assess sampler convergence, the empirical sample covariance $\\mathbf{S}_y^{(k)}$ was calculated for each iteration k using $10^4$ chains of samples.\n",
    "* We reproduce a following figure by using our implementations. The figure shows a convergence of $\\textbf{Var}(\\mathbf{y}^{(k)}) \\to \\mathbf{A}^{-1}$. For the detail implementaion, see `\"../results/graph-sampler.jl\"`\n",
    "\n",
    "<center><img src=\"img/figure2.png\"></center>\n",
    "\n",
    "* $y_1$ denotes Cheby-SSOR with w = 1 and $y_2$ denotes Cheby-SSOR with w = 1.6441. $y_3$ is SSOR with w = 1 and $y_4$ is SSOR with w = 1.6441. Finally, $y_5$ is non-iterative cholesky sampler of algorithm 1 in the paper. Below is the figure from the paper.\n",
    "\n",
    "<center><img src=\"img/figure2paper.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Real Data Analysis: Biofilm Image Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Motivation\n",
    "- In this section we deal with a A.Parker et al.'s JASA paper [Polynomial Accelerated Solutions to a Large Gaussian Model for Imaging Biofilms: In Theory and Finite Precision](https://doi.org/10.1080/01621459.2017.1409121). Because of hardness of the problem in our original paper, we choose the author's latter research, which includes our research.\n",
    "\n",
    "- The object of this problem is sampling a $512 \\times 512$ size biofilm image, which requires sampling from normal distribution with variance of a $512^2 \\times 512^2$ matrix. The 6-2 example of the 2017 study even used a $10^6 \\times 10^6$ matrix.\n",
    "\n",
    "- The Chebyshev Accelerated Sampler needs to get an extreme eigenvalues as input variables. The problem is that attaining eigenvalues are demanding when the size of the matrix becomes larger. The 2017 study did not cover this, but mention that they obtained the extreme eigenvalues, $\\lambda_{max}, \\lambda_{min}$ using the cg-sampler. \n",
    "\n",
    "- Then how can we use the cg sampler to find the extreme eigenvalues? This requires to look at the series of studies done by Colin Fox and Albert Parker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Authors| Key of the paper |\n",
    "|-------------------------------|-------------------------------------------------------------------|\n",
    "| Fox, C., and Parker, A.(2012) | Introduced a conjugate gradient sampler which then could inexpensively estimate some of the eigenvalues of $A$.|\n",
    "| Fox, C., and Parker, A.(2014) | First suggested a Cheby-accelerated sampler.|\n",
    "| __Fox, C., and Parker, A.(2017)__ | Proved convergence in distribution for Gibbs samplers corresponding to _any_ matrix splitting and accelerated by _any_ polynomial that is independent of the Gibbs iteration.|\n",
    "| Parker, A. et al.(2018) | Combined all this methods and proposed PCG and __PCG-Chebyshev Accelerated Sampler__.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solving linear inverse problem for large matrices requires to obtain an extreme eigen values of $M_{SSOR}^{-1}A$.\n",
    "\n",
    "* The most important parts are well documented in the 2018 paper. The crucial fact that PCG sampler accoplishes are\n",
    "    - The PCG sampler with preconditiner equal to the splitting matrix $M = M_1M_1^T$ provides an avenue to estimate the extreme eigenvalues of $M^{-1}A$ that are required by Chebyshev. A $k \\times k$ tridiagonal matrix is built from the PCG parameters and the eigenvalues of this tridiagonal, found at a negligible $k^2$ flops when $k<<n$, are the required extreme eigenvalues of $M^{-1}A $. (2018, p1437)\n",
    "    - PCG provides an estimate of the mean $\\mu = A^{-1}b \\neq 0$.The PCG sampler is used to perform the mean calculation because PCG is a faster linear solver than Chebyshev and will find μ after a finite number of iterations. Put another way, the Chebyshev sampler can sample from $N(0,A−1)$ much faster compared to sampling from the Normal with nonzero mean.\n",
    "    \n",
    "* So far, in order to adapt our sampler to a larger size problems, we implemented the followings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Paper| What we implemented |\n",
    "|-------------------------------|-------------------------------------------------------------------|\n",
    "| Fox, C., and Parker, A.(2012) | CG Accelerated Sampler (_pcg_sampler.jl_)|\n",
    "| Fox, C., and Parker, A.(2014) | Chebyshev-accelerated sampler(_cheby_sampler.jl_)|\n",
    "| __Fox, C., and Parker, A.(2017)__ | Chebyshev-accelerated sampler(_cheby_sampler.jl_)|\n",
    "| Parker, A. et al.(2018) | PCG Accelerated Sampler, PCG-Chebyshev Accelerated Sampler(_pcg_cheby.jl_)|\n",
    "| Additional |Generating Lanczos matrix(lemma 2.1 in paper 2012, _pcg_cheby.jl_), Generating generalized lattice sparse precision matrix( EX 6.2 in paper 2017 and EX in 2018 , _matrixgen.jl_)|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data setting\n",
    "\n",
    "- Confocal microscopes (CM) capture a set of planar “slices” or images, parallel to, and at different distances from, the bottom of the biofilm\n",
    "  where it is attached to a surface. In this paper, they analyze a sequence of CM images over time of a green fluorescing *Staphylococcus aureus* biofilm grown under controlled conditions.\n",
    "\n",
    "- The data set is given by a video file(see [supplementary material](https://www.tandfonline.com/doi/suppl/10.1080/01621459.2017.1409121?scroll=top)). In the video file, we use a first 40 frames of biofilm images about 10 minute. Following image is a 1st frame of the video file.\n",
    "\n",
    "<img src=\"img/frame_1.png\" width=\"500\" height=\"600\" />\n",
    "\n",
    "- Biofilm captures $620 \\mu m \\times 620 \\mu m$ planar with a vertical range of $112 \\mu m$. Vertical value means the observed height of Staphylococcus aureus at (x, y) pixel point. The 3D pixelation is $512 \\times 512 \\times 17$ pixels with a $512 \\times 512$ pixel representation for each planar slice. (즉, 박테리아 군집을 높이에 따라 17개의 slicing 단면을 촬영한 정보를 한 frame으로 합쳐놓았다고 생각하면 된다.) After resizing, it becomes like as following.\n",
    "\n",
    "<img src=\"img/gframe_1.png\" width=\"300\" height=\"300\" />\n",
    "\n",
    "#### 2. Modeling\n",
    "- Note that we can consider a height information as a discretize observed surface representation. We want to recover the whole surface information $\\theta$.\n",
    "\n",
    "- From the observed The linear statistical model that we apply to the surface profile is\n",
    "  $$y = F\\theta + \\epsilon.$$\n",
    "  \n",
    "- The random vector $y \\in \\mathbb{R}^{512 \\times 512}$ representation of the biofilm surface calculated from the CM image. $\\theta$ is the true biofilm surface that we want to estimate. $F$ is a blurring of the surface, $\\epsilon \\sim N(0, \\Sigma_y)$. The likelihood is $\\pi(y|\\theta, \\Sigma_y) = N(F\\theta, \\Sigma_y)$. Let's introduce the prior $\\pi(\\theta) = N(0, \\frac{1}{\\lambda}W^{-1})$ with an unknow parameter $\\lambda$ from the smoothness assumption. $W$ is a $512^2 \\times 512^2$ precision matrix defined from the Section 6.1 $\\mathbf{A}$ with $n = 512$. \n",
    "\n",
    "- After some Bayesian perspective calculation procedures, it reduces to sample from\n",
    "  $$\n",
    "  \\pi(\\theta|y, \\sigma^2, \\lambda) = N(\\frac{1}{\\sigma^2}A^{-1}y, A^{-1})\n",
    "  $$\n",
    "  with precision matrix $A = \\frac{1}{\\sigma^2}I + \\lambda W$ and $1/\\sigma^2, \\lambda \\sim Gamma(\\alpha = 1, \\beta = 10^{-4})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implementation\n",
    "\n",
    "- 2018 JASA paper uses iterative samplers from large multivariate Gaussians to reconstruct bioflim surfaces and to estimate the effect of the treatment on the biofim's volume. Here, we will use CSLM images to reconstruct biofilm surfaces using the accelerated chebyshev-ssor sampler we implemented. \n",
    " \n",
    "##### The need for PCG - Chebyshev Accelerated Sampler\n",
    "- Finding eigenvalues is not easy for large dimensional matrices. Since our implementation of chebyshev-ssor requires extreme eigenvalues, $\\lambda_{max}, \\lambda_{min}$, for $512 ^ 2 \\times 512 ^ 2$ sized matrix, this computation is very demanding. `Arpack.jl`'s `eigs` fails to calculate a extreme eigen value of these matrices for high-dimensional case.\n",
    "- The PCG-Chebyshev algorithm proposed in the 2018 JASA paper that makes this problem easier. We can create a Lanczos tridiagonal matrix using a set of values, $\\{\\gamma_k, \\beta_k\\}$, attained during PCG sampling and it is known that the extreme eigenvalues of $M^{-1}A$ that are required by Chebyshev are the extreme eigenvalues of this $k \\times k$(here, $k$ is the number of iterations) tridiagonal matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented all the samplers, but found that the kernel dies in running all at once using pcg_cheby function. Instead, the results could be obtained by sequentially applying pcg sampler-> eigenvalue extraction-> Chebyshev sampler. To see the process, see \"data_run.ipynb\"\n",
    "\n",
    "The paper procured samples by perfoming _$10^4$_ iterations of sampling and then averaging them for each fame in the video. This took about 2.5 hours of running on each frame for them. Running out of time, we instead only sampled _once_ and produced the result, which would generate a large variation. \n",
    "\n",
    "Further, we used the minimum value of the frame picture that we used as the reference as the scale value, and as changing this, the values in the graph can change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sample once (our result)\n",
    "\n",
    "<img src=\"img/run_1_times.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "* sample 10000th and averages (from paper)|\n",
    "\n",
    "<img src=\"img/result_from_paper.png\" width = \"500\" height=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe further research will prove that our sampler is effective for large sampling problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Refrences\n",
    "- Fox Colin, and Albert Parker. “Accelerated Gibbs sampling of normal distributions using matrix splittings and polynomials.” Bernoulli 23.4B (2017): 3711-3743.\n",
    "- Albert Parker et al. \"Polynomial Accelerated Solutions to a Large Gaussian Model for Imaging Biofilms: In Theory and Finite Precision.\", Journal of the American Statistical Association Volume 113, 2018 - Issue 524.\n",
    "- Owe Axelsson, Iterative Solution Methods, Cambridge, 1994.\n",
    "- IterativeSolver.jl, `julia` package, https://github.com/JuliaMath/IterativeSolvers.jl."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
